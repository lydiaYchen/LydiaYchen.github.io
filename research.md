---
layout: page
title: Papers with code
---

## TabWak: A Watermark for Tabular Diffusion Models

Chaoyi Zhu, Jeroen Galjaard, Pin-Yu Chen, Lydia Y. Chen

### **ICLR 2025 (üèÜ Spotlight):** [üìÑ Paper](https://openreview.net/pdf?id=71pur4y8gs) | [üíª Code](https://github.com/chaoyitud/TabWak) | [üñºÔ∏è Poster](https://iclr.cc/media/PosterPDFs/ICLR%202025/30853.png)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{zhu2025tabwak,
  author    = {Chaoyi Zhu and
               Jiayi Tang and
               Jeroen M. Galjaard and
               Pin{-}Yu Chen and
               Robert Birke and
               Cornelis Bos and
               Lydia Y. Chen},
  title     = {TabWak: A Watermark for Tabular Diffusion Models},
  booktitle = {The Thirteenth International Conference on Learning Representations, {ICLR} 2025},
  publisher = {OpenReview.net},
  year      = {2025}
}

```
</details>

<br>

## Duwak: Dual Watermarks in Large Language Models

Chaoyi Zhu, Jeroen Galjaard, Pin-Yu Chen, Lydia Y. Chen

### **ACL 2024:** [üìÑ Paper](https://aclanthology.org/2024.findings-acl.678.pdf) | [üíª Code](https://github.com/chaoyitud/Dual-Watermarks)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{zhu2024duwak,
    author      = {Chaoyi Zhu and
                   Jeroen M. Galjaard and
                   Pin{-}Yu Chen and
                   Lydia Y. Chen},
    title       = {Duwak: Dual Watermarks in Large Language Models},
    booktitle   = {Findings of the Association for Computational Linguistics: ACL 2024},
    publisher   = {Association for Computational Linguistics},
    year        = {2024},
    doi         = {10.18653/v1/2024.findings-acl.678}
}
```
</details>

<br>

## WaveStitch: Flexible and Fast Conditional Time Series Generation with Diffusion Models

### [üìÑ Paper](https://arxiv.org/pdf/2503.06231) | [üíª Code](https://github.com/adis98/HierarchicalTS)

<details>
<summary>Citation</summary>

```bibtex
@article{shankar2025wavestitch,
    author  = {Aditya Shankar and
               Lydia Y. Chen and
               Arie van Deursen and
               Rihan Hai},
    title   = {WaveStitch: Flexible and Fast Conditional Time Series Generation with Diffusion Models},
    journal = {CoRR},
    volume  = {abs/2503.06231},
    year    = {2025}
}
```
</details>

<br>

## Federated Time Series Generation on Feature and Temporally Misaligned Data

Zhi Wen Soi, Chenrui Fan, Aditya Shankar, Abele MƒÉlan, Lydia Y. Chen

### **ECML 2025:** [üìÑ Paper](https://arxiv.org/pdf/2410.21072) | [üíª Code](https://github.com/soizhiwen/FedTDD)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{soi2025fedtdd,
    author      = {Zhi Wen Soi and
                   Chenrui Fan and
                   Aditya Shankar and
                   Abele MƒÉlan and
                   Lydia Y. Chen},
    title       = {Federated Time Series Generation on Feature and Temporally Misaligned Data},
    booktitle   = {Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference, {ECML} {PKDD} 2025},
    year        = {2025}
}
```
</details>

<br>

## CCBNet: Confidential Collaborative Bayesian Networks Inference

Abele MƒÉlan, Thiago Guzella, J√©r√©mie Decouchant, Lydia Y. Chen

### **FC 2025:** [üìÑ Paper](https://fc25.ifca.ai/preproceedings/129.pdf) | [üíª Code](https://github.com/AbeleMM/ccbnet)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{malan2025ccbnet,
    author      = {Abele MƒÉlan and
                   Thiago Guzella and
                   J√©r√©mie Decouchant and
                   Lydia Chen},
    title       = {CCBNet: Confidential Collaborative Bayesian Networks Inference},
    booktitle   = {Financial Cryptography and Data Security - 29th International Conference, {FC} 2025},
    series      = {Lecture Notes in Computer Science},
    publisher   = {Springer},
    year        = {2025},
}
```
</details>

<br>

## TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models

Caspar Meijer, Jiyue Huang, Shreshtha Sharma, Elena Lazovik, Lydia Y. Chen

### **SaTML 2025:** [üìÑ Paper](https://fc25.ifca.ai/preproceedings/129.pdf) | [üíª Code](https://github.com/Capsar/ts-inverse)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{meijer2025tsinverse,
    author      = {Caspar Meijer and
                   Jiyue Huang and
                   Shreshtha Sharma and
                   Elena Lazovik and
                   Lydia Y. Chen},
    title       = {TS-Inverse: {A} Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models},
    booktitle   = {IEEE Conference on Secure and Trustworthy Machine Learning, SaTML 2025},
    publisher   = {IEEE},
    year        = {2025},
    doi         = {10.1109/SATML64287.2025.00014}
}
```
</details>

<br>

## Share Secrets for Privacy: Confidential Forecasting with Vertical Federated Learning

Aditya Shankar, J√©r√©mie Decouchant Decouchant, Dimitra Gkorou, Rihan Hai, Lydia Y. Chen

### **ARES 2025:** [üìÑ Paper](https://arxiv.org/pdf/2405.20761) | [üíª Code](https://github.com/adis98/STV)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{shankar2025stv,
    author      = {Aditya Shankar and
                   J√©r√©mie Decouchant and
                   Dimitra Gkorou and
                   Rihan Hai and
                   Lydia Y. Chen},
    title       = {Share Secrets for Privacy: Confidential Forecasting with Vertical Federated Learning},
    booktitle   = {Proceedings of the 19th International Conference on Availability, Reliability and Security, {ARES} 2025},
    publisher   = {{ACM}},
    year        = {2025}
}
```
</details>

<br>

## TabuLa: Harnessing Language Models for Tabular Data Synthesis

Zilong Zhao, Robert Birke, Lydia Chen

### **PAKDD 2025:** [üìÑ Paper](https://arxiv.org/pdf/2310.12746) | [üíª Code](https://github.com/zhao-zilong/Tabula)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{zhao2025stv,
    author      = {Zilong Zhao and
                   Robert Birke and
                   Lydia Y. Chen},
    title       = {TabuLa: Harnessing Language Models for Tabular Data Synthesis},
    booktitle   = {Advances in Knowledge Discovery and Data Mining - 29th Pacific-Asia Conference on Knowledge Discovery and Data Mining, {PAKDD} 2025},
    series      = {Lecture Notes in Computer Science},
    publisher   = {Springer},
    year        = {2025},
    doi         = {10.1007/978-981-96-8186-0\_20}
}
```
</details>

<br>

## BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise.

Jeroen Galjaard, Robert Birke, Juan P√©rez, and Lydia Y. chen

### **ECML 2025:** [üìÑ Paper](https://github.com/JMGaljaard/batman-clr-noisy-meta-learning) | [üíª Code](https://github.com/JMGaljaard/batman-clr-noisy-meta-learning)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{soi2025fedtdd,
    author      = {Jeroen Galjaard  and
                   Robert Birke and
                   Juan Perez and
                   Lydia Y. Chen},
    title       = {BatMan-CLR: Making Few-shots Meta-Learners Resilient Against Label Noise.},
    booktitle   = {Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference, {ECML} {PKDD} 2025},
    year        = {2025}
}
```
</details>

<br>




## SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks

Nikolay Blagoev, Lydia Y. Chen, Oƒüuzhan Ersoy

### [üìÑ Paper](https://arxiv.org/pdf/2502.19913) | [üíª Code](https://github.com/gensyn-ai/skippipe)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{blagoev2025skippipe,
    author  = {Nikolay Blagoev and
                Lydia Yiyu Chen and
                Oguzhan Ersoy},
    title   = {SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks},
    journal = {CoRR},
    volume  = {abs/2502.19913},
    year    = {2025}
}
```
</details>

<br>

## LeadFL: Client Self-Defense against Model Poisoning in Federated Learning

Chaoyi Zhu, Stefanie Roos, Lydia Y. Chen

### **ICML 2023:** [üìÑ Paper](https://proceedings.mlr.press/v202/zhu23j/zhu23j.pdf) | [üíª Code](https://github.com/chaoyitud/LeadFL) | [üñºÔ∏è Poster](https://icml.cc/media/PosterPDFs/ICML%202023/24161.png)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{zhu2023leadfl,
    author      = {Chaoyi Zhu and
                   Stefanie Roos and
                   Lydia Y. Chen},
    title       = {LeadFL: Client Self-Defense against Model Poisoning in Federated Learning},
    booktitle   = {International Conference on Machine Learning, {ICML} 2023},
    series      = {Proceedings of Machine Learning Research},
    publisher   = {{PMLR}},
    year        = {2023}
}
```
</details>

<br>

## 	Gradient Inversion of Federated Diffusion Models

Jiyue Huang, Chi Hong, Stefanie Roos and Lydia Y. Chen

### **ARES 2025:** [üìÑ Paper](https://github.com/GillHuang-Xtler/GIDM_diffusion_inversion) | [üíª Code](https://github.com/GillHuang-Xtler/GIDM_diffusion_inversion)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{shankar2025stv,
    author      = {Jiyue Huang and
                  Chi Hong and
                   Stefanie Ross and
                   Lydia Y. Chen},
    title       = {Gradient Inversion of Federated Diffusion Models},
    booktitle   = {Proceedings of the 19th International Conference on Availability, Reliability and Security, {ARES} 2025},
    publisher   = {{ACM}},
    year        = {2025}
}
```
</details>

<br>


## On Quantifying the Gradient Inversion Risk of Data Reuse in Federated Learning Systems

Jiyue Huang, Lydia Y. Chen, Stefanie Roos

### **SRDS 2024:** [üìÑ Paper](https://github.com/GillHuang-Xtler/GIDM_diffusion_inversion) | [üíª Code](https://github.com/GillHuang-Xtler/CGI_multiserver_inversion)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{huang2024cgi,
    author      = {Jiyue Huang and
                   Lydia Y. Chen and
                   Stefanie Roos},
    title       = {On Quantifying the Gradient Inversion Risk of Data Reuse in Federated Learning Systems},
    booktitle   = {43rd International Symposium on Reliable Distributed Systems, {SRDS} 2024},
    publisher   = {IEEE},
    year        = {2024},
    doi         = {10.1109/SRDS64841.2024.00031}
}
```
</details>

<br>

## Fabricated Flips: Poisoning Federated Learning without Data

Jiyue Huang, Zilong Zhao, Lydia Y. Chen, Stefanie Roos

### **DSN 2023:** [üìÑ Paper](https://arxiv.org/pdf/2202.05877) | [üíª Code](https://github.com/GillHuang-Xtler/DFA_untargeted_attack)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{huang2024dfa,
    author      = {Jiyue Huang and
                   Zilong Zhao and
                   Lydia Y. Chen and
                   Stefanie Roos},
    title       = {Fabricated Flips: Poisoning Federated Learning without Data},
    booktitle   = {53rd Annual {IEEE/IFIP} International Conference on Dependable Systems and Network, {DSN} 2023},
    publisher   = {{IEEE}},
    year        = {2023},
    doi         = {10.1109/DSN58367.2023.00036}
}
```
</details>

<br>

## CTAB-GAN: Effective Table Data Synthesizing

Zilong Zhao, Aditya Kunar, Hiek Van der Scheer, Robert Birke, Lydia Y. Chen

### **ACML 2021:** [üìÑ Paper](https://arxiv.org/pdf/2102.08369) | [üíª Code](https://github.com/Team-TUD/CTAB-GAN)

<details>
<summary>Citation</summary>

```bibtex
@inproceedings{zhao2021ctabgan,
    author      = {Zilong Zhao and
                   Aditya Kunar and
                   Robert Birke and
                   Lydia Y. Chen},
    title       = {CTAB-GAN: Effective Table Data Synthesizing},
    booktitle   = {Asian Conference on Machine Learning, {ACML} 2021},
    series      = {Proceedings of Machine Learning Research},
    publisher   = {{PMLR}},
    year        = {2021}
}
```
</details>

<!--
Our research themes span in the following areas.

- [Generative Models](#generative-models)
- [Robust, and Private Learning](#robust-and-private-learning)
- [Federated Learning ](#federated-learning-)

# Generative Models<a name="Generative"></a>

While big data is powering up the deep learning models, it is costly and inevitably intrudes privacy to curate such data. Synthetically generated data not only alleviates the cost of collecting data but also overcome the privacy concerns and legislation boundary. How to generate synthetic data that fulfill the requirements of data similarity, analysis utility, privacy and generalization?

We are exploring a wide range of generative models for synthesizing tabular data, ranging from Generative Adversarial Networks (GANs), latent difussion, flow models, and large language models.
We are also actively collaborating with various industrial partners to explore synthetic data as a privacy-preserving data sharing solution, such as major European energy companies, and finacial companies.

# Robust, and Private Learning<a name="RPFlearning"></a>

Artificial intelligence (AI) and machine learning (ML) are ubiquitous in our daily lives in the form of search engines, machine translation, self-driving cars and much more. The prevailing assumptions of existing ML algorithms are that data is neutral and can be freely accessed (without breaching privacy). As a result, the existing algorithms fall short of addressing challenges in realistic scenarios, i.e., against adversarial examples, dirty data, and unreliable execution environments while still preserving data privacy. These issues are further exacerbated by large and distributed learning problems, the data for which is collected over multiple sources and must be computed on distributed nodes.

In this line of research, we are designing robust, privacy-preserving and fair learning algorithms. Topics include:
- Robust Machine Learning: designing learning algorithms that are robust to dirty data inputs.
- Adversarial Attacks and Defenses: designing adversarial attacks and defense mechanisms for deployed deep models.
- Differential private (deep) learning: designing effective differential private ML models with precise accuracy accounting.

<figure>
 <a href="#top">
  <img src="../assets/img/top.png" alt="top" style="float: right;" width="30" height="30">
 </a>
</figure>

# Federated Learning <a name="eLInf"></a>
Data is constantly generated and collected by edge devices (of the network) to power up today‚Äôs AI and ML analyses. With the advancement of algorithmic compression techniques and hardware technology, the ability to train neural networks and run inference on edge devices has gone from myth to reality. Federated learning (FL) is an emerging learning paradigm where distributed edge nodes collaboratively learn the weights of neural networks iteratively without directly sharing data. It is largely unexplored how existing deep learning algorithms can be realized within a FL framework, thereby overcoming network communications and adversarial threats. Moreover, owing to the vast number of available trained models and highly heterogeneous mobile devices, it is no mean feat to identify and deploy the right model for individual edge devices.

In this line of research, we are designing learning algorithms and prototyping system solutions for ML training and inference on distributed edge devices. Topics include:

- Confidential Vertical Learning for Manufacturer: collaborating with the world leading material manufacturers to design confidential vertical federated learning on variety of machine learning models
- Attacks and Defenses in Federated Learning: designing data free model poisoning attacks, gradient inversion attacks, and freerider attacks for various federated learning systems
- Continue Federated Learning and Domain Adaptation: designing federated learning systems that tackle two foundemntal challenges in real life: data continitously evolves through different domains and learning tasks also change over time.
- Deep Model Inferences on Edge Devices: designing and prototyping an inference engine that can search for optimal models and configurations for edge devices at scale.

<figure>
 <a href="#top">
  <img src="../assets/img/top.png" alt="top" style="float: right;" width="30" height="30">
 </a>
</figure>
-->
