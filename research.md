---
layout: page
title: Papers with code
---

<a name="top"></a> 



### 📄 TabWak: A Watermark for Tabular Diffusion Models
*ICLR 2025 (Spotlight)*  🏆

💻 **Code:** [GitHub](https://github.com/chaoyitud/TabWak)  
 <details><summary>Show citation</summary>
```bibtex
@inproceedings{zhu2025tabwak,
  title={TabWak: A Watermark for Tabular Diffusion Models},
  author={Zhu, Chaoyi and Tang, Jiayi and Galjaard, Jeroen M. and Chen, Pin-Yu and Birke, Robert and Bos, Cornelis and Chen, Lydia Y.},
  booktitle={International Conference on Learning Representations},
  year={2025},
  note={Spotlight}
}
```
</details>

<br>


### 📄 Duwak: Dual Watermarks in Large Language Models  
*ACL 2024*  
💻 **Code:** [GitHub](https://github.com/chaoyitud/Dual-Watermarks)
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{duwak2024,
  title={Duwak: Dual Watermarks in Large Language Models},
  author={},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2024}
}
```
</details>

<br>

### 📄 WaveStitch: Flexible and Fast Conditional Time Series Generation with Diffusion Models 
💻 **Code:** [GitHub](https://github.com/adis98/HierarchicalTS)  
<details>
<summary>Show citation</summary>
```bibtex
@article{shankar2025wavestitch,
  title={WaveStitch: Flexible and Fast Conditional Time Series Generation with Diffusion Models},
  author={Shankar, A. and Chen, Lydia Y. and van Deursen, A. and Hai, R.},
  journal={CoRR},
  volume={abs/2503.06231},
  year={2025}
}
```
</details>
<br>


### 📄 Federated Time Series Generation on Feature and Temporally Misaligned Data  
*ECML 2025*  
💻 **Code:** [GitHub](https://github.com/soizhiwen/FedTDD)  
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{fedtdd2025,
  title={Federated Time Series Generation on Feature and Temporally Misaligned Data},
  author={},
  booktitle={European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases},
  year={2025}
}
```
</details>
<br>

### 📄 CCBNet: Confidential Collaborative Bayesian Networks Inference. 
💻 **Code:** [GitHub](https://github.com/adis98/STV)  
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{Malan25FCCCBNet,
  title={CCBNet: Confidential Collaborative Bayesian Networks Inference},
  author={Abele Malan and
                  J{\'{e}}r{\'{e}}mie Decouchant and
                  Thiago Guzella and
                  Lydia Y. Chen},
  booktile={Finacnail Crypto and Data Engineering},
  year={2025}
}
```
</details>
<br>

### 📄 TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models
*SaTML 2025*  
💻 **Code:** [GitHub](https://github.com/Capsar/ts-inverse)  
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{meijer2025tsinverse,
  title={TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models},
  author={Meijer, C. and Huang, J. and Sharma, S. and Lazovik, E. and Chen, Lydia Y.},
  booktitle={IEEE Conference on Secure and Trustworthy Machine Learning},
  year={2025}
}
```
</details>

### 📄 Share Secrets for Privacy: Confidential Forecasting with Vertical Federated Learning**  
*ARES 2025*  
💻 **Code:** [GitHub](https://github.com/adis98/STV)
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{Shankar25ARES,
  title={Share Your Secrets for Privacy! Confidential Forecasting with Vertical
                  Federated Learning},
  author={Aditya Shankar and
                  J{\'{e}}r{\'{e}}mie Decouchant and
                  Dimitra Gkorou and
                  Rihan Hai and
                Lydia Y. Chen},
  booktitle={20th International Conference on Availability, Reliability and Security,
  year={2025}
}
```
</details>
<br>

hare Secrets for Privacy: Confidential Forecasting with Vertical Federated Learning

### 📄  GTV: Generating Tabular Data via Vertical Federated Learning
*DSN 2025*  
💻 **Code:** [GitHub](https://github.com/zhao-zilong/gtv)
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{tv2025,
  title={TV: Generating Tabular Data via Vertical Federated Learning},
  author={},
  booktitle={IEEE/IFIP International Conference on Dependable Systems and Networks},
  year={2025}
}
```
</details>
<br>

### 📄 TabuLa: Harnessing Language Models for Tabular Data Synthesis 
*PAKDD25*
💻 **Code:** [GitHub](https://github.com/zhao-zilong/Tabula)  
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{tabula2025,
  title={TabuLa: Harnessing Language Models for Tabular Data Synthesis},
  author={},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  year={2025}
}
```
</details>
<br>

### 📄 SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks  
💻 **Code:** [GitHub](https://github.com/gensyn-ai/skippipe)  
 <details>
<summary>Show citation</summary>
```bibtex
@article{blagoev2025skippipe,
  title={SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks},
  author={Blagoev, N. and Chen, Lydia Y. and Ersoy, O.},
  journal={CoRR},
  volume={abs/2502.19913},
  year={2025}
}
```
</details>
<br>





### 📄 LeadFL: Client Self-Defense against Model Poisoning in Federated Learning.
*ICML 2023*  
💻 **Code:** [GitHub](https://github.com/chaoyitud/LeadFL)  
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{ZhuICML23FL,
  author       = {Chaoyi Zhu and
                  Stefanie Roos and
                  Lydia Y. Chen},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {LeadFL: Client Self-Defense against Model Poisoning in Federated Learning},
  booktitle    = {International Conference on Machine Learning},
  volume       = {202},
}
```
</details>
<br>


### 📄 On Quantifying the Gradient Inversion Risk of Data Reuse in Federated Learning Systems
*SRDS 2024*  
💻 **Code:** [GitHub](https://github.com/GillHuang-Xtler/CGI_multiserver_inversion)  
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{huang2024quantifying,
  title={On Quantifying the Gradient Inversion Risk of Data Reuse in Federated Learning Systems},
  author={Huang, J. and Chen, Lydia Y. and Roos, S.},
  booktitle={International Symposium on Reliable Distributed Systems},
  year={2024}
}
```
</details>
<br>


### 📄 Fabricated Flips: Poisoning Federated Learning without Data
*DSN 2023*  
💻 **Code:** [GitHub](https://github.com/GillHuang-Xtler/DFA_untargeted_attack)  
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{huang2023fabricated,
  title={Fabricated Flips: Poisoning Federated Learning without Data},
  author={Huang, J. and Zhao, Z. and Chen, Lydia Y. and Roos, S.},
  booktitle={Annual IEEE/IFIP International Conference on Dependable Systems and Networks},
  year={2023}
}
```
</details>
<br>

### 📄 CTAB-GAN: Effective Table Data Synthesizing
*ACML 2021*  
💻 **Code:** [GitHub](https://github.com/Team-TUD/CTAB-GAN)  
<details>
<summary>Show citation</summary>
```bibtex
@inproceedings{zhao2021ctabgan,
  title={CTAB-GAN: Effective Table Data Synthesizing},
  author={Zhao, Z. and Kunar, A. and Birke, R. and Chen, Lydia Y.},
  booktitle={Asian Conference on Machine Learning},
  year={2021},
  editor={Balasubramanian, V. N. and Tsang, I. W.}
}
```
</details>

<br>

<!--
Our research themes span in the following areas. 

- [Generative Models](#generative-models)
- [Robust, and Private Learning](#robust-and-private-learning)
- [Federated Learning ](#federated-learning-)
  

# Generative Models<a name="Generative"></a>

While big data is powering up the deep learning models, it is costly and inevitably intrudes privacy to curate such data. Synthetically generated data not only alleviates the cost of collecting data but also overcome the privacy concerns and legislation boundary. How to generate synthetic data that fulfill the requirements of data similarity, analysis utility, privacy and generalization?

We are exploring a wide range of generative models for synthesizing tabular data, ranging from Generative Adversarial Networks (GANs), latent difussion, flow models, and large language models. 
We are also actively collaborating with various industrial partners to explore synthetic data as a privacy-preserving data sharing solution, such as major European energy companies, and finacial companies. 



# Robust, and Private Learning<a name="RPFlearning"></a> 

Artificial intelligence (AI) and machine learning (ML) are ubiquitous in our daily lives in the form of search engines, machine translation, self-driving cars and much more. The prevailing assumptions of existing ML algorithms are that data is neutral and can be freely accessed (without breaching privacy). As a result, the existing algorithms fall short of addressing challenges in realistic scenarios, i.e., against adversarial examples, dirty data, and unreliable execution environments while still preserving data privacy. These issues are further exacerbated by large and distributed learning problems, the data for which is collected over multiple sources and must be computed on distributed nodes.

In this line of research, we are designing robust, privacy-preserving and fair learning algorithms. Topics include:
- Robust Machine Learning: designing learning algorithms that are robust to dirty data inputs.
- Adversarial Attacks and Defenses: designing adversarial attacks and defense mechanisms for deployed deep models.
- Differential private (deep) learning: designing effective differential private ML models with precise accuracy accounting.

<figure>
 <a href="#top">
  <img src="../assets/img/top.png" alt="top" style="float: right;" width="30" height="30">
 </a>
</figure>

# Federated Learning <a name="eLInf"></a> 
Data is constantly generated and collected by edge devices (of the network) to power up today’s AI and ML analyses. With the advancement of algorithmic compression techniques and hardware technology, the ability to train neural networks and run inference on edge devices has gone from myth to reality. Federated learning (FL) is an emerging learning paradigm where distributed edge nodes collaboratively learn the weights of neural networks iteratively without directly sharing data. It is largely unexplored how existing deep learning algorithms can be realized within a FL framework, thereby overcoming network communications and adversarial threats. Moreover, owing to the vast number of available trained models and highly heterogeneous mobile devices, it is no mean feat to identify and deploy the right model for individual edge devices.

In this line of research, we are designing learning algorithms and prototyping system solutions for ML training and inference on distributed edge devices. Topics include:

- Confidential Vertical Learning for Manufacturer: collaborating with the world leading material manufacturers to design confidential vertical federated learning on variety of machine learning models
- Attacks and Defenses in Federated Learning: designing data free model poisoning attacks, gradient inversion attacks, and freerider attacks for various federated learning systems
- Continue Federated Learning and Domain Adaptation: designing federated learning systems that tackle two foundemntal challenges in real life: data continitously evolves through different domains and learning tasks also change over time. 
- Deep Model Inferences on Edge Devices: designing and prototyping an inference engine that can search for optimal models and configurations for edge devices at scale.

<figure>
 <a href="#top">
  <img src="../assets/img/top.png" alt="top" style="float: right;" width="30" height="30">
 </a>
</figure>

-->
